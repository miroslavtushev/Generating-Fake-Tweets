{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Fake Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes an end-to-end NLP project where I generate fake tweets based on an existing dataset.\n",
    "\n",
    "For this project I picked a massive dump of Trump's tweets as the dataset is free and availabe [here](https://www.thetrumparchive.com/faq).\n",
    "\n",
    "**Our objective is to generate believable fake tweets.**\n",
    "\n",
    "Two models are used in this notebook: Markov Chains and LSTM. These models are trained and empirically evaluated. Markov Chain was able to generate simple, but predictable tweets. Two NN architectures of LSTM were tested. The results showed that both architectures generate better fake tweets than Markov Chains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>isRetweet</th>\n",
       "      <th>isDeleted</th>\n",
       "      <th>device</th>\n",
       "      <th>favorites</th>\n",
       "      <th>retweets</th>\n",
       "      <th>date</th>\n",
       "      <th>isFlagged</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1205186844276727809</th>\n",
       "      <td>RT @RepAndyBiggsAZ: This President has done a ...</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>0</td>\n",
       "      <td>9291</td>\n",
       "      <td>2019-12-12 18:05:09</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>903778130850131970</th>\n",
       "      <td>On behalf of @FLOTUS Melania &amp;amp, myself, THA...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>53160</td>\n",
       "      <td>10516</td>\n",
       "      <td>2017-09-02 00:34:32</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>349858983609249793</th>\n",
       "      <td>\"\"\"@cloverc8: @realDonaldTrump that's because ...</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>8</td>\n",
       "      <td>11</td>\n",
       "      <td>2013-06-26 11:57:36</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1190000118398963713</th>\n",
       "      <td>RT @RepMeuser: Today’s vote on the impeachment...</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>0</td>\n",
       "      <td>8010</td>\n",
       "      <td>2019-10-31 20:18:32</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1295067207618244610</th>\n",
       "      <td>Thank you James! https://t.co/cKiKlAXHqI</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>Twitter for iPhone</td>\n",
       "      <td>104805</td>\n",
       "      <td>15667</td>\n",
       "      <td>2020-08-16 18:37:38</td>\n",
       "      <td>f</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text  \\\n",
       "id                                                                       \n",
       "1205186844276727809  RT @RepAndyBiggsAZ: This President has done a ...   \n",
       "903778130850131970   On behalf of @FLOTUS Melania &amp, myself, THA...   \n",
       "349858983609249793   \"\"\"@cloverc8: @realDonaldTrump that's because ...   \n",
       "1190000118398963713  RT @RepMeuser: Today’s vote on the impeachment...   \n",
       "1295067207618244610           Thank you James! https://t.co/cKiKlAXHqI   \n",
       "\n",
       "                    isRetweet isDeleted               device  favorites  \\\n",
       "id                                                                        \n",
       "1205186844276727809         t         f   Twitter for iPhone          0   \n",
       "903778130850131970          f         f   Twitter for iPhone      53160   \n",
       "349858983609249793          f         f  Twitter for Android          8   \n",
       "1190000118398963713         t         f   Twitter for iPhone          0   \n",
       "1295067207618244610         f         f   Twitter for iPhone     104805   \n",
       "\n",
       "                     retweets                date isFlagged  \n",
       "id                                                           \n",
       "1205186844276727809      9291 2019-12-12 18:05:09         f  \n",
       "903778130850131970      10516 2017-09-02 00:34:32         f  \n",
       "349858983609249793         11 2013-06-26 11:57:36         f  \n",
       "1190000118398963713      8010 2019-10-31 20:18:32         f  \n",
       "1295067207618244610     15667 2020-08-16 18:37:38         f  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"tweets_01-08-2021.csv\", index_col=\"id\", parse_dates=['date'])\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56571, 8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For text generation to be useful, we need to preserve the original words' forms. Thereforem stemming or lemmatization is not needed here. Punctuation might also be useful, so each symbol will be treated as a separate token.\n",
    "\n",
    "It is a good idea though to remove hyperlinks and convert the dataset to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "\n",
    "text = data.text.to_list()\n",
    "processed_tweets = []\n",
    "tokenizer = RegexpTokenizer('\\w+|\\S+')\n",
    "for tweet in text:\n",
    "    tweet = re.sub('(https?:[\\w\\/\\.\\d]+)|…|(^RT)|“|”|\"', \"\", tweet)\n",
    "    tweet = re.sub(\"&amp;?\", \"and\", tweet)\n",
    "    processed_tweets.append(tokenizer.tokenize(tweet.lower()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['getting', 'a', 'little', 'exercise', 'this', 'morning', '!']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tweets[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.histplot([len(tweet) for tweet in processed_tweets], bins=15)\n",
    "plt.xlabel(\"Tweet length\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the tweets tend to be of the length 20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chains: Attempt 1\n",
    "\n",
    "I coded the algorithm from scratch using the [following resource](https://www.kdnuggets.com/2019/11/markov-chains-train-text-generation.html) as a guide. To get a more detailed information on how it works, please refer to my [Medium](https://miroslavtushev.medium.com/generating-fake-trump-tweets-using-markov-chains-d75d9a2eed08) article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chains of 2 words\n",
    "k = 2\n",
    "# list to hold those chains\n",
    "sets_of_k_words = []\n",
    "\n",
    "# create chains with k-length\n",
    "for tweet in processed_tweets:\n",
    "    sets_of_k_words.append([' '.join(tweet[i:i+k]) for i, _ in enumerate(tweet[:-k+1])])\n",
    "\n",
    "from scipy.sparse import dok_matrix\n",
    "# how many unique chains are there?\n",
    "distinct_sets_of_k_words = list(set([chain for s in sets_of_k_words for chain in s]))\n",
    "# how many unique words are there?\n",
    "distinct_words = list(set([elem for l in processed_tweets for elem in l]))\n",
    "# sparse matrix: rows - k-length sequence, columns - all possible words in tweets\n",
    "next_after_k_words_matrix = dok_matrix((len(distinct_sets_of_k_words), len(distinct_words)), dtype=np.uint16)\n",
    "\n",
    "# to look up the index of a chain (row) for the matrix\n",
    "k_words_idx_dict = {word: i for i, word in enumerate(distinct_sets_of_k_words)}\n",
    "# to look up the index of a word (column) for the matrix\n",
    "word_idx_dict = {word: i for i, word in enumerate(distinct_words)}\n",
    "\n",
    "# for each sequence go over all tweets and find the next word\n",
    "# increment the count for that word\n",
    "for i, set_of_k_words in enumerate(sets_of_k_words):\n",
    "    for j, k_word in enumerate(set_of_k_words[:-k+1]):\n",
    "        # index for a row (chain)\n",
    "        word_sequence_idx = k_words_idx_dict[k_word]\n",
    "        # get the index for next w\n",
    "        next_word_idx = word_idx_dict[processed_tweets[i][j+k]]\n",
    "        next_after_k_words_matrix[word_sequence_idx, next_word_idx] +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple Markov Chain text generator is completely deterministic in nature. That is, there is not randomness component and the transition to the next state happens according to the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple case with 0 alpha and fixed length\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from numpy.random import choice\n",
    "\n",
    "def sample_next_word_after_sequence(word_sequence, alpha = 0):\n",
    "    next_word_vector = next_after_k_words_matrix[k_words_idx_dict[word_sequence]] + alpha\n",
    "    likelihoods = csr_matrix(next_word_vector)/next_word_vector.sum()\n",
    "    weights = likelihoods.toarray().flatten()\n",
    "    # if no words possible - terminate\n",
    "    if weights.sum() == 0.0:\n",
    "        return \"\"\n",
    "    return choice(distinct_words, p=weights)\n",
    "    \n",
    "def stochastic_chain(seed, chain_length=10, seed_length=k):\n",
    "    current_words = seed.split(' ')\n",
    "    if len(current_words) != seed_length:\n",
    "        raise ValueError(f'wrong number of words, expected {seed_length}')\n",
    "    sentence = seed\n",
    "\n",
    "    for _ in range(chain_length):\n",
    "        sentence += ' '\n",
    "        next_word = sample_next_word_after_sequence(' '.join(current_words))\n",
    "        if next_word == \"\":\n",
    "            return sentence\n",
    "        sentence += next_word\n",
    "        current_words = current_words[1:]+[next_word]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shit out . borders big problem for which the corrupt new york , doug ! thanks\n",
      "teamwork and coordination at all times . achievement is not a group\n",
      "@utennjock: @realdonaldtrump please run . what do\n"
     ]
    }
   ],
   "source": [
    "print(stochastic_chain(choice(distinct_sets_of_k_words), chain_length=15))\n",
    "print(stochastic_chain(choice(distinct_sets_of_k_words), chain_length=10))\n",
    "print(stochastic_chain(choice(distinct_sets_of_k_words), chain_length=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problems:**\n",
    "- The current implementation is deterministic, so there is very little variety in tweets, and common patterns are often appear.\n",
    "- Periods are not taken into account, so the tweets end abruptly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chains: Attempt 2\n",
    "\n",
    "Here we are going to address the above-mentioned problems by (a) introducing randomness in the selection of the next state, and (b) adding tweet termination based on the probability of the current length. \n",
    "\n",
    "Let's start with the latter. To correctly terminate a generated tweet that looks believable, we are going to generate a CDF of each length probability. Then, when generating a new tweet token by tokn if we encounter a punctuation symbol like *.?!*, we are going to sample the CDF with the probability of terminating the tweet based on the current length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# generating CDF\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lens = np.sort([len(tweet) for tweet in processed_tweets])\n",
    "print(lens[-1])\n",
    "\n",
    "def gen_prob(val, lens=lens):\n",
    "    for i, elem in enumerate(lens):\n",
    "        if elem >= val:\n",
    "            return i / len(lens)\n",
    "    return 1.00\n",
    " \n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(np.cumsum(lens) / np.cumsum(lens)[-1])\n",
    "ax.set_title(\"CDF for tweet length\")\n",
    "ax.set_xlim(0, len(lens))\n",
    "ax.set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address the lack of randomness problem, we introduced the new parameter **alpha**. Alpha represents a small probabilty that a new state transition happens completely randomly, by picking a new state among all states, instead of the ones with the highest probability. By increasing alpha we can generate truly bizarre tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import random\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "def sample_next_word_after_sequence(word_sequence, alpha = 0):\n",
    "    # generate a random word by chance\n",
    "    if random() <= alpha:\n",
    "        return distinct_words[choice(len(distinct_words)-1)]\n",
    "    next_word_vector = next_after_k_words_matrix[k_words_idx_dict[word_sequence]]\n",
    "    likelihoods = csr_matrix(next_word_vector)/next_word_vector.sum()\n",
    "    weights = likelihoods.toarray().flatten()\n",
    "    if weights.sum() == 0.0:\n",
    "        return \"\"\n",
    "    return choice(distinct_words, p=weights)\n",
    "\n",
    "def stochastic_chain(seed, alpha=0):\n",
    "    # if only 1 word provided\n",
    "    if len(seed.split(' ')) != k:\n",
    "        # complete the chain\n",
    "        possible_words = [s for s in distinct_sets_of_k_words if s.startswith(seed)]\n",
    "        seed = choice(possible_words)           \n",
    "    current_words = seed.split(' ')  \n",
    "    sentence = seed\n",
    "\n",
    "    while(1):\n",
    "        sentence += ' '\n",
    "        next_word = sample_next_word_after_sequence(' '.join(current_words), alpha)\n",
    "        if next_word == \"\":\n",
    "            return postprocess(sentence)\n",
    "        elif next_word in list(\".!?\"):\n",
    "            sentence += next_word\n",
    "            if random() <= gen_prob(len(sentence.split())):\n",
    "                return postprocess(sentence)\n",
    "        else:\n",
    "            sentence += next_word\n",
    "        current_words = current_words[1:]+[next_word]\n",
    "        \n",
    "def postprocess(sent):\n",
    "    final = []\n",
    "    for s in sent_tokenize(sent):\n",
    "        final.append(re.sub(r\" ([.?!])$\", r\"\\1\", s).capitalize())\n",
    "    return \" \".join(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter vicious censorship still hitting on me. Remember and vote republican.\n",
      "Democrats if they 're coming armed with the most elite property true luxury.\n",
      "Obamacare throughout the year @brandenroderick returns to the senate democrats have never had such leadership in space. Plans can change the rules and regulations.\n"
     ]
    }
   ],
   "source": [
    "print(stochastic_chain('twitter', alpha=0.001))\n",
    "print(stochastic_chain('democrats', alpha=0.001))\n",
    "print(stochastic_chain('obama', alpha=0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- Increasing the chain length (attempt 1) leads to more coherent sentences. However, it limits the number of possible states and requires a much larger dataset.\n",
    "- Adding randomness (alpha) and termination probability further improves the quality of generated tweets. However, the interpretability of the generated tweets is often low as Markov Chains does not provide a mechanism for conveying meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM\n",
    "\n",
    "To overcome the limitations of Markov Chains, especially the lack of meaning, we are going to build a simple NN model using LSTM. As NNs can represent arbitrarily complex relationships, my expectation is that the performance is going to be much better compared to simple probabilistic models like Markov Chains. The complete theory behind LSTM can be found in my [Medium](https://miroslavtushev.medium.com/generating-fake-trump-tweets-with-lstm-7c5979229e81) article.\n",
    "\n",
    "The core idea behind LSTM is the following: ***given a sequence of n words, what is the most likely word to follow?***\n",
    "\n",
    "To realize that idea, we are going to generate sequences of tokens and their corresponding labels. We first generate unique ids for each token, and then iterate over our tweets to generate all possible sequences of ids starting from the beginning of a tweet. The labels are simply the last word in a sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# generating sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# keeping punctuation\n",
    "tok = Tokenizer(filters=\"\")\n",
    "tok.fit_on_texts(processed_tweets)\n",
    "\n",
    "sequences = []\n",
    "\n",
    "for seq in processed_tweets:\n",
    "    tokenized = tok.texts_to_sequences([seq])[0]\n",
    "    for i in range(2, len(tokenized)+1):\n",
    "        sequences.append(tokenized[:i])\n",
    "        \n",
    "total_words = len(tok.word_index) + 1\n",
    "\n",
    "labels = []\n",
    "for sequence in sequences:\n",
    "    labels.append(sequence.pop(-1))\n",
    "\n",
    "labels = np.array(labels)#.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 6343, 1065, 10, 1018, 794, 9, 7, 438, 1385, 5, 25, 2418, 3, 6, 2, 92, 140, 16, 1, 541, 36, 1272, 1386, 872] 27\n"
     ]
    }
   ],
   "source": [
    "print(sequences[100], labels[100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the sequence above, the most likely token to follow is 27. \n",
    "\n",
    "To standardize the lengths of the sequences, the shorter onces (<100) will be padded with zeroes. Other strategies exist like dropping the long ones to save space and computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "X = np.array(pad_sequences(\n",
    "    sequences, padding=\"post\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1\n",
    "The first model is composed of the following layers:\n",
    "1. **Embedding layer.** Take a sequence and output an embedding of the shape (99, 256). Each token is represented by a vector of 256 numerical values.\n",
    "2. **LSTM layer.** Take a 3d tensor of the shape (4096, 99, 256), with dropout 0.3.\n",
    "3. **Dense layer.** Output a prediction (probability) of the next word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "batch_size = 4096\n",
    "\n",
    "model1 = keras.Sequential([\n",
    "    layers.Embedding(input_dim=total_words, output_dim=256, mask_zero=True),\n",
    "    layers.LSTM(units=256, dropout=0.3, stateful=False),\n",
    "    layers.Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "model1.compile(loss='sparse_categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "291/291 [==============================] - 97s 293ms/step - loss: 8.1393\n",
      "\n",
      "Epoch 00001: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 2/30\n",
      "291/291 [==============================] - 86s 295ms/step - loss: 7.0362\n",
      "\n",
      "Epoch 00002: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 3/30\n",
      "291/291 [==============================] - 89s 306ms/step - loss: 6.8504\n",
      "\n",
      "Epoch 00003: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 4/30\n",
      "291/291 [==============================] - 89s 306ms/step - loss: 6.6731\n",
      "\n",
      "Epoch 00004: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 5/30\n",
      "291/291 [==============================] - 89s 306ms/step - loss: 6.4333\n",
      "\n",
      "Epoch 00005: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 6/30\n",
      "291/291 [==============================] - 88s 302ms/step - loss: 6.1923\n",
      "\n",
      "Epoch 00006: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 7/30\n",
      "291/291 [==============================] - 89s 305ms/step - loss: 5.9695\n",
      "\n",
      "Epoch 00007: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 8/30\n",
      "291/291 [==============================] - 90s 309ms/step - loss: 5.7934\n",
      "\n",
      "Epoch 00008: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 9/30\n",
      "291/291 [==============================] - 92s 317ms/step - loss: 5.6318\n",
      "\n",
      "Epoch 00009: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 10/30\n",
      "291/291 [==============================] - 96s 331ms/step - loss: 5.4857\n",
      "\n",
      "Epoch 00010: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 11/30\n",
      "291/291 [==============================] - 96s 330ms/step - loss: 5.3612\n",
      "\n",
      "Epoch 00011: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 12/30\n",
      "291/291 [==============================] - 94s 324ms/step - loss: 5.2388\n",
      "\n",
      "Epoch 00012: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 13/30\n",
      "291/291 [==============================] - 90s 310ms/step - loss: 5.1263\n",
      "\n",
      "Epoch 00013: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 14/30\n",
      "291/291 [==============================] - 87s 298ms/step - loss: 5.0307\n",
      "\n",
      "Epoch 00014: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 15/30\n",
      "291/291 [==============================] - 86s 294ms/step - loss: 4.9214\n",
      "\n",
      "Epoch 00015: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 16/30\n",
      "291/291 [==============================] - 86s 295ms/step - loss: 4.8416\n",
      "\n",
      "Epoch 00016: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 17/30\n",
      "291/291 [==============================] - 86s 294ms/step - loss: 4.7574\n",
      "\n",
      "Epoch 00017: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 18/30\n",
      "291/291 [==============================] - 86s 297ms/step - loss: 4.6804\n",
      "\n",
      "Epoch 00018: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 19/30\n",
      "291/291 [==============================] - 87s 299ms/step - loss: 4.6024\n",
      "\n",
      "Epoch 00019: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 20/30\n",
      "291/291 [==============================] - 86s 294ms/step - loss: 4.5342\n",
      "\n",
      "Epoch 00020: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 21/30\n",
      "291/291 [==============================] - 85s 293ms/step - loss: 4.4628\n",
      "\n",
      "Epoch 00021: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 22/30\n",
      "291/291 [==============================] - 86s 297ms/step - loss: 4.4015\n",
      "\n",
      "Epoch 00022: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 23/30\n",
      "291/291 [==============================] - 87s 297ms/step - loss: 4.3375\n",
      "\n",
      "Epoch 00023: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 24/30\n",
      "291/291 [==============================] - 86s 297ms/step - loss: 4.2832\n",
      "\n",
      "Epoch 00024: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 25/30\n",
      "291/291 [==============================] - 85s 294ms/step - loss: 4.2222\n",
      "\n",
      "Epoch 00025: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 26/30\n",
      "291/291 [==============================] - 86s 294ms/step - loss: 4.1728\n",
      "\n",
      "Epoch 00026: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 27/30\n",
      "291/291 [==============================] - 86s 294ms/step - loss: 4.1224\n",
      "\n",
      "Epoch 00027: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 28/30\n",
      "291/291 [==============================] - 86s 297ms/step - loss: 4.0758\n",
      "\n",
      "Epoch 00028: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 29/30\n",
      "291/291 [==============================] - 85s 291ms/step - loss: 4.0317\n",
      "\n",
      "Epoch 00029: saving model to checkpoints\\model1.ckpt\n",
      "Epoch 30/30\n",
      "291/291 [==============================] - 84s 288ms/step - loss: 3.9876\n",
      "\n",
      "Epoch 00030: saving model to checkpoints\\model1.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x169cf34bfa0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"checkpoints\\\\model1.ckpt\",\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "model1.fit(X, labels, \n",
    "          epochs=30, \n",
    "          batch_size=batch_size, \n",
    "          verbose=True, \n",
    "          use_multiprocessing=True, \n",
    "          workers=8,\n",
    "          callbacks=[cp_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions, we create a reverse-lookup dictionary ids and tokens. We select a seed to start with and then continuously predict subsequent tokens based on the required length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_word = {v:k for k,v in tok.word_index.items()}\n",
    "\n",
    "def predict_sequence(seed, length, model):\n",
    "    # how much to pad\n",
    "    max_len = len(X[0])\n",
    "    # final prediction sequence\n",
    "    output = seed + \" \"\n",
    "    # keep predicting until reach the length\n",
    "    for i in range(length): \n",
    "        # tokenize\n",
    "        tokenized = tok.texts_to_sequences([output])[0]\n",
    "        # pad\n",
    "        x = np.array(pad_sequences([tokenized], padding=\"post\", maxlen=max_len))\n",
    "        # predict based on current sequence\n",
    "        prediction = np.argmax(model.predict(x, verbose=False), axis=-1)\n",
    "        # update sequence\n",
    "        output = output + id_to_word[prediction[0]] + \" \"\n",
    "        if id_to_word[prediction[0]] in list(\".!?\"):\n",
    "            if random() <= gen_prob(len(output.split())):\n",
    "                return postprocess(output)\n",
    "    return postprocess(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter is a very good story. The fake news media is not a very bad for the people of the united states. They are not a very good and , and , the fake news media is now doing.\n",
      "Democrats are trying to be a wall and , the wall , and the wall is not a very good for our country.\n",
      "Obama 's debt is a disaster for the u .s. Senate. He is a great guy. He is a great guy.\n",
      "Biden is a very big mistake in the united states. We are going to win the united states. We will win! #maga #kag #maga #kag #kag #kag agenda is the best!\n",
      "Republicans are doing a great job for the people of the united states.\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence('twitter', 100, model1))\n",
    "print(predict_sequence('democrats', 100, model1))\n",
    "print(predict_sequence('obama', 100, model1))\n",
    "print(predict_sequence('biden', 100, model1))\n",
    "print(predict_sequence('republicans', 100, model1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2\n",
    "The second model adds an additional LSTM layer without droput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1162/1162 [==============================] - 190s 157ms/step - loss: 7.5284\n",
      "\n",
      "Epoch 00001: saving model to checkpoints2\\model2.ckpt\n",
      "Epoch 2/20\n",
      "1162/1162 [==============================] - 192s 165ms/step - loss: 6.8265\n",
      "\n",
      "Epoch 00002: saving model to checkpoints2\\model2.ckpt\n",
      "Epoch 3/20\n",
      "1162/1162 [==============================] - 190s 163ms/step - loss: 6.0302\n",
      "\n",
      "Epoch 00003: saving model to checkpoints2\\model2.ckpt\n",
      "Epoch 4/20\n",
      "1162/1162 [==============================] - 189s 163ms/step - loss: 5.4678\n",
      "\n",
      "Epoch 00004: saving model to checkpoints2\\model2.ckpt\n",
      "Epoch 5/20\n",
      "1162/1162 [==============================] - 191s 164ms/step - loss: 5.0896\n",
      "\n",
      "Epoch 00005: saving model to checkpoints2\\model2.ckpt\n",
      "Epoch 6/20\n",
      "1162/1162 [==============================] - 184s 159ms/step - loss: 4.7873\n",
      "\n",
      "Epoch 00006: saving model to checkpoints2\\model2.ckpt\n",
      "Epoch 7/20\n",
      "1162/1162 [==============================] - 188s 162ms/step - loss: 4.5443\n",
      "\n",
      "Epoch 00007: saving model to checkpoints2\\model2.ckpt\n",
      "Epoch 8/20\n",
      "1162/1162 [==============================] - 192s 165ms/step - loss: 4.3060\n",
      "\n",
      "Epoch 00008: saving model to checkpoints2\\model2.ckpt\n",
      "Epoch 9/20\n",
      "1162/1162 [==============================] - 186s 160ms/step - loss: 4.1018\n",
      "\n",
      "Epoch 00009: saving model to checkpoints2\\model2.ckpt\n",
      "Epoch 10/20\n",
      "1162/1162 [==============================] - 183s 158ms/step - loss: 3.9103\n",
      "\n",
      "Epoch 00010: saving model to checkpoints2\\model2.ckpt\n",
      "Epoch 11/20\n",
      "1162/1162 [==============================] - 182s 157ms/step - loss: 3.7366\n",
      "\n",
      "Epoch 00011: saving model to checkpoints2\\model2.ckpt\n",
      "Epoch 12/20\n",
      "1162/1162 [==============================] - 204s 175ms/step - loss: 3.5792\n",
      "\n",
      "Epoch 00012: saving model to checkpoints2\\model2.ckpt\n",
      "Epoch 13/20\n",
      "1162/1162 [==============================] - 188s 161ms/step - loss: 3.4449\n",
      "\n",
      "Epoch 00013: saving model to checkpoints2\\model2.ckpt\n",
      "Epoch 14/20\n",
      "1162/1162 [==============================] - 187s 161ms/step - loss: 3.3173\n",
      "\n",
      "Epoch 00014: saving model to checkpoints2\\model2.ckpt\n",
      "Epoch 15/20\n",
      "1162/1162 [==============================] - 188s 162ms/step - loss: 3.1987\n",
      "\n",
      "Epoch 00015: saving model to checkpoints2\\model2.ckpt\n",
      "Epoch 16/20\n",
      "1162/1162 [==============================] - 182s 157ms/step - loss: 3.0922\n",
      "\n",
      "Epoch 00016: saving model to checkpoints2\\model2.ckpt\n",
      "Epoch 17/20\n",
      "1162/1162 [==============================] - 185s 159ms/step - loss: 2.9919\n",
      "\n",
      "Epoch 00017: saving model to checkpoints2\\model2.ckpt\n",
      "Epoch 18/20\n",
      "1162/1162 [==============================] - 186s 160ms/step - loss: 2.8939\n",
      "\n",
      "Epoch 00018: saving model to checkpoints2\\model2.ckpt\n",
      "Epoch 19/20\n",
      "1162/1162 [==============================] - 186s 160ms/step - loss: 2.8113\n",
      "\n",
      "Epoch 00019: saving model to checkpoints2\\model2.ckpt\n",
      "Epoch 20/20\n",
      "1162/1162 [==============================] - 183s 158ms/step - loss: 2.7195\n",
      "\n",
      "Epoch 00020: saving model to checkpoints2\\model2.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16c01588610>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model 2\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "batch_size = 1024\n",
    "\n",
    "model2 = keras.Sequential([\n",
    "    layers.Embedding(input_dim=total_words, output_dim=256, mask_zero=True),\n",
    "    layers.LSTM(units=256, return_sequences=True),\n",
    "    layers.LSTM(units=256),\n",
    "    layers.Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"checkpoints2/model2.ckpt\",\n",
    "                                                 save_weights_only=True,\n",
    "                                                 verbose=1)\n",
    "\n",
    "model2.fit(X, labels, \n",
    "          epochs=20, \n",
    "          batch_size=batch_size, \n",
    "          verbose=True, \n",
    "          use_multiprocessing=True, \n",
    "          workers=8,\n",
    "          callbacks=[cp_callback],\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter is going to be able to render the economy in the history of our country. It is a big setback for growth.\n",
      "Democrats are now doing everything possible to make a deal with the dems. They are trying to steal the election.\n",
      "Obama is spending a job on the economy. He is a true champion and has been a true winner of character. He is a true champion and supporter!\n",
      "Biden is a corrupt politician who has been a complete and total disaster , and he is now doing nothing to do with the radical left democrats in congress.\n",
      "Republicans should get a new (repeal and , concise. It is time to get tough and smart! The dems have gone crazy.\n"
     ]
    }
   ],
   "source": [
    "print(predict_sequence('twitter', 100, model2))\n",
    "print(predict_sequence('democrats', 100, model2))\n",
    "print(predict_sequence('obama', 100, model2))\n",
    "print(predict_sequence('biden', 100, model2))\n",
    "print(predict_sequence('republicans', 100, model2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Observations:***\n",
    "- Model 2 was able to reach lower loss value measured by cross entropy compared to model 1, in less iterations.\n",
    "- The quality of the fake tweets does not seem to improve significantly, although Model 2 produces wordier tweets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "While Markov Chains provided a computationally easy way to generate fake tweets, the majority of them lack any coherent meaning. LSTM, on the other hand, was able to produce much better results. The fake tweets produced by LSTM are much more meaningful and believable. \n",
    "\n",
    "# Future Improvements\n",
    "- Garbage in — garbage out. 90% of success stems from good data. A more careful preprocessing can be done. For instance, you can try to remove hashtags since I found that predictions always go into a \"vicious circle\" of hashtags when the model doesn’t know what to predict. It will simply output a ton a unrelated hashtags, which obviously doesn’t have a lot of value. Another thing to try would be drop tweets with too low or too high length.\n",
    "- Model architecture. I was hoping to achieve better results with a deeper NN with less units, but apparently shallower, wider NN worked better for me. You can experiment with the # of layers, # of units, and dropout rate.\n",
    "- Replace Embedding layer with the actual word embeddings trained on your dataset, such as Glove or Word2vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
